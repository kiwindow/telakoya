{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "dZGzoyVHjdep",
        "mPLwF0d30ck7",
        "n3A62Ym47vXD",
        "x_kzi0ZA7x36",
        "McAGloto9-KO",
        "jAyXVt4cBjHY",
        "XTb0BjxMC58F",
        "jC0-C3hJGjDY",
        "72RpjncYGmuE",
        "xZkDqBp9m9kE",
        "vk_nAALAnRj3",
        "z-YccPmus6cL",
        "H6T1EvGjsUpa",
        "ZwFldizVu3Lw",
        "vdRvP3pzu3Lw",
        "GL_fA49lymZu",
        "AW7ExxKKzg5v",
        "AkmYSXfF3ExU",
        "bLshO9Gr5h4H",
        "6qZWqpGVnSXT",
        "LrBl5lGd6lIv",
        "BqdDeC2Q6o0c",
        "_zIswdNe6xiQ",
        "YmUZIlkv7m-3",
        "5HLmeo_A8Yu7",
        "jMqO20Oa9WsZ",
        "HYXivXvt-D1m",
        "rXAxrNCk-NtS",
        "BQjzHzND-kt8",
        "gH2NERPl-n00",
        "76P20KzQ-qZq",
        "M-Gv27So-tUr",
        "RBUnAOf4-wYG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ■ 日本腎・血液浄化AI学会主催「よくわかる寺子屋セミナー」\n",
        "\n",
        "# 第5回「深層学習による画像問題」　演習プログラム 【重要なCommandのみ】"
      ],
      "metadata": {
        "id": "7EewZ4vjix3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## □ 文責　学術委員会副委員長　岩藤和広"
      ],
      "metadata": {
        "id": "ObWUzCOajAZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. 環境設定"
      ],
      "metadata": {
        "id": "dZGzoyVHjdep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MyDriveの中にTelakoya というフォルダーを作成し、その中に tela5 というフォルダーを作成する\n",
        "- フォルダー tela5 の中に model というフォルダーを作る。"
      ],
      "metadata": {
        "id": "mXHT_P34jgR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 最初に、このnotebookをGoogle Driveにmount(Google driveに接続)する。\n",
        "- その方法は、https://github.com/kiwindow/telakoya の中の「Google Colabortoryの使い方」を参照。\n",
        "- 【重要】 mountする前に\n",
        "- 「ランタイム」➡「ランタイムのタイプを変更」➡「ハードウェアアクセラレータ」を「GPU」に切り替えて保存する。\n",
        "- 　これを行わないと、エラーが出たり、計算に長い時間がかかる。\n",
        "- mount出来ると、左のサイドバーに drive というフォルダーが現れる。\n",
        "- もし drive というフォルダーがなければ、次のセルを実行する。"
      ],
      "metadata": {
        "id": "fLeKGSG7Aais"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gogle driveにmountする\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eAvyB44qA0JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 以下のコマンドでどの種類のGPUに接続したかが分かる\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "HFw-SoKt4Sy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ライブラリやデータの準備"
      ],
      "metadata": {
        "id": "mPLwF0d30ck7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) ライブラリの読み込み"
      ],
      "metadata": {
        "id": "n3A62Ym47vXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tensorflow2のwrapperであるKerasを利用する。\n",
        "- 深層学習はTensorflow2かPyTorchというframeworkを利用するが、どちらもプログラムがやや煩雑。\n",
        "- KerasはTensorflow2を機械学習のsklearnのような書式で書けるようにしたもの。\n",
        "- Tensorflow2は企業などで、PyTorchは研究機関などで、それぞれ利用されることが多い。"
      ],
      "metadata": {
        "id": "ZjTomyr-8qa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリの読み込み\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np  # 数値計算のライブラリ\n",
        "import matplotlib.pyplot as plt  # 図示のライブラリ\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "cQlCR_oDj4Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) データの読み込み"
      ],
      "metadata": {
        "id": "x_kzi0ZA7x36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mnist と呼ばれる手書き数字(0〜9)の画像データを利用する。\n",
        "- mnist には、学習データが60,000字、検証データが10,000字ある。\n",
        "- データは、28×28個の整数(0〜255)からなる行列で形成されている。\n",
        "- 一つ一つの文字には、0〜9の正解データも付いている。"
      ],
      "metadata": {
        "id": "xPtB14Uz72tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "from keras.datasets import mnist\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "SLeN1gZl7oNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) 画像データの正規化・浮動小数点化とChannelの次元の付加"
      ],
      "metadata": {
        "id": "McAGloto9-KO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 整数のデータを、0〜1の浮動小数点(float32)に正規化する。\n",
        "- 個々のデータの形を、(縦の数、横の数、Channel)にする。"
      ],
      "metadata": {
        "id": "v0xMXivI-Emt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像データの正規化(0〜1)と小数化(float32)\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255"
      ],
      "metadata": {
        "id": "Vk_10Lim96bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html"
      ],
      "metadata": {
        "id": "4vWiNdvOCsGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 個々のデータの形を 28 x 28 x 1 とする (channelの次元を最後に加える)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "metadata": {
        "id": "1EHOPseiANMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) 正解データをOne Hot vectorにする"
      ],
      "metadata": {
        "id": "jAyXVt4cBjHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keras.io/ja/utils/#to_categorical"
      ],
      "metadata": {
        "id": "CSrCPfq9CgwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- モデルの損失関数が Categorical Crossentropy で、分類結果毎の確率が出る場合、正解データを One Hot vectorにする必要がある。"
      ],
      "metadata": {
        "id": "o4koUTPYGILG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "t_train = to_categorical(t_train, 10)\n",
        "t_test = to_categorical(t_test, 10)"
      ],
      "metadata": {
        "id": "HptFrU1eBGhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 三層のPerceptronによる画像分類"
      ],
      "metadata": {
        "id": "XTb0BjxMC58F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) モデルの設計"
      ],
      "metadata": {
        "id": "jC0-C3hJGjDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/guide/keras/sequential_model?hl=ja"
      ],
      "metadata": {
        "id": "r8geXd-1rnGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Networkを形成するモジュールの読み込み\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "jyPw4MYBCG7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 三層のPerceptronの作製。中間層の活性化関数はRelu、出力層の活性化関数はsoftmaxとする。\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28,28,1)),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "Lg71_1c3EDty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの構成の確認\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "fZiMseJnF9_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) モデルの学習"
      ],
      "metadata": {
        "id": "72RpjncYGmuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/guide/keras/train_and_evaluate?hl=ja"
      ],
      "metadata": {
        "id": "L9eYozVzr8yX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 損失関数が Categorical Crossentropyの場合、正解データはOne Hot vectorにする必要がある。"
      ],
      "metadata": {
        "id": "kWz541tR1hJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 人間が読み書きする source code を、機会が読み書きする machine code にコンパイルする。\n",
        "# その際、1) 最適化アルゴリズム、2) 損失関数、3) 評価指標なども設定する。\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "O5aziFobGSxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの学習を実行する。trainとかfitと呼ばれるmethodで実行する。\n",
        "# その際、1) training data : validation dataの比　2) バッチサイズ 3) エポック数も設定する。\n",
        "history = model.fit(x_train, t_train, validation_split = 0.2, batch_size = 128, epochs = 10)"
      ],
      "metadata": {
        "id": "MwXzqYLmJYMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習過程の図示\n",
        "\n",
        "# Access the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot of the training and validation loss\n",
        "epochs = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot　of the training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NzmWekfGRjES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) モデルの検証"
      ],
      "metadata": {
        "id": "xZkDqBp9m9kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/guide/keras/train_and_evaluate?hl=ja"
      ],
      "metadata": {
        "id": "MUmEEEVkraOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測モデルの精度の検証\n",
        "score = model.evaluate(x_test, t_test, verbose = 0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])"
      ],
      "metadata": {
        "id": "2JxeYtJimt1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) モデルの保存と復元"
      ],
      "metadata": {
        "id": "vk_nAALAnRj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/guide/keras/save_and_serialize?hl=ja"
      ],
      "metadata": {
        "id": "0MaCX3hFqyLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存。HDF5ファイルで保存される。\n",
        "model.save('/content/drive/MyDrive/Telakoya/tela5/model/perceptron.h5')"
      ],
      "metadata": {
        "id": "Lr8Nv90PnE4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの復元。\n",
        "from keras.models import load_model\n",
        "model2 = load_model('/content/drive/MyDrive/Telakoya/tela5/model/perceptron.h5')"
      ],
      "metadata": {
        "id": "w5X1lX17nl_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存前のモデルによる予測結果\n",
        "pred = model.predict(x_test[0:1])\n",
        "pred"
      ],
      "metadata": {
        "id": "NHdyFxH7pdaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 復元したモデルによる予測結果\n",
        "pred2 = model2.predict(x_test[0:1])\n",
        "pred2"
      ],
      "metadata": {
        "id": "jEuFhwtophvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 補題 Fashion mnistによる画像分類"
      ],
      "metadata": {
        "id": "z-YccPmus6cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Googe driveへのマウント。既に行われている場合は評価は不要。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8Xqfpe1-pH1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/keras/classification?hl=ja"
      ],
      "metadata": {
        "id": "iVcx6xK8tEqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fashion mnistというデータセットがあり、mnistと同じ構造で作られている。\n",
        "- これまでおこなってきたPerceptronによるモデルで、Fashion mnistも分類できる。\n",
        "- 以下のように、読み込むデータを入れ替えるだけでよい\n"
      ],
      "metadata": {
        "id": "4PVHj9qXtHHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fashion mnistの読み込み\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train, t_train), (x_test, t_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "RGLVZ5BXtXEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fashion mnistのデータの前処理\n",
        "\n",
        "# データの正規化 (0〜1)\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "# データの形を 28 x 28 x 1 にする (channelを加える)\n",
        "import numpy as np\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# 正解データをone hot vectorにする\n",
        "t_train = keras.utils.to_categorical(t_train, 10)\n",
        "t_test = keras.utils.to_categorical(t_test, 10)\n",
        "\n",
        "# 10種類の画像\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "metadata": {
        "id": "3Af5Zv00tinA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初の10個の画像の図示\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.title('Label: ' + str(i))\n",
        "    plt.imshow(x_train[i])"
      ],
      "metadata": {
        "id": "2Y23AlfwtvS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Networkを形成するモジュールの読み込み\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "# 三層のPerceptronの作製。中間層の活性化関数はRelu、出力層の活性化関数はsoftmaxとする。\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28,28,1)),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# モデルの構成の確認\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "I1yoLwucf9PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 人間が読み書きする source code を、機会が読み書きする machine code にコンパイルする。\n",
        "# その際、1) 最適化アルゴリズム、2) 損失関数、3) 評価指標なども設定する。\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# モデルの学習を実行する。trainとかfitと呼ばれるmethodで実行する。\n",
        "# その際、1) training data : validation dataの比　2) バッチサイズ 3) エポック数も設定する。\n",
        "history = model.fit(x_train, t_train, validation_split = 0.2, batch_size = 128, epochs = 10)\n",
        "\n",
        "# 学習過程の図示\n",
        "\n",
        "# Access the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot of the training and validation loss\n",
        "epochs = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot　of the training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PUVIslvogIBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データでモデルの予測精度を検証する\n",
        "score = model.evaluate(x_test, t_test, verbose = 0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])"
      ],
      "metadata": {
        "id": "lIL2brSQgYCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存。HDF5ファイルで保存される。\n",
        "model.save('/content/drive/MyDrive/Telakoya/tela5/model/perceptronFM.h5')"
      ],
      "metadata": {
        "id": "2Q-MhzL3gY95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの復元。\n",
        "from keras.models import load_model\n",
        "model2 = load_model('/content/drive/MyDrive/Telakoya/tela5/model/perceptronFM.h5')"
      ],
      "metadata": {
        "id": "7CoatRuIgfVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存前のモデルによる予測結果\n",
        "pred = model.predict(x_test[0:1])\n",
        "pred"
      ],
      "metadata": {
        "id": "Xvi9Y3PbgjgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 復元したモデルによる予測結果\n",
        "pred2 = model2.predict(x_test[0:1])\n",
        "pred2"
      ],
      "metadata": {
        "id": "dou1e8vzgoal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. CNNによる画像分類"
      ],
      "metadata": {
        "id": "H6T1EvGjsUpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/images/cnn?hl=ja"
      ],
      "metadata": {
        "id": "sBLhcThyurbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Googe driveへのマウント。既に行われている場合は評価は不要。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AJBl-vFzqyPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) ライブラリの読み込み"
      ],
      "metadata": {
        "id": "ZwFldizVu3Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tensorflow2のwrapperであるKerasを利用する。\n"
      ],
      "metadata": {
        "id": "_Civ99qQu3Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリの読み込み\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "import numpy as np  # 数値計算のライブラリ\n",
        "import matplotlib.pyplot as plt  # 図示のライブラリ\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "N3YLfARPu3Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) データの読み込み"
      ],
      "metadata": {
        "id": "vdRvP3pzu3Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CIFAR10 と呼ばれるカラー画像データを利用する。\n",
        "- CIFAR10 には、学習データが60,000字、検証データが10,000字ある。\n",
        "- データは、32×32個の整数(0〜255)が3組ある行列(テンソル)で形成されている。\n",
        "- 一つ一つの文字には、0〜9の正解データも付いている。"
      ],
      "metadata": {
        "id": "fbvUaR50u3Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "GYjvBq2Au3Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- モデルの損失関数がSparse Categorical Crossentropyの場合、正解データは一次元でよい。"
      ],
      "metadata": {
        "id": "3jNQu6EhyVFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初の25個の画像の図示\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i])\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qUgaQDrLwFny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) モデルの設計"
      ],
      "metadata": {
        "id": "GL_fA49lymZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN (Convolutional Neural Network)の構造。最初に空のSequentialをインスタンス化し、それにLayerを加えてゆく形を取っている。\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "# モデルのアーキテクチャ(構造)の表示\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wstISitHwlmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) モデルの学習"
      ],
      "metadata": {
        "id": "AW7ExxKKzg5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのコンパイル\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "E_7fGxMPyxEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの学習\n",
        "history = model.fit(train_images, train_labels, epochs=10, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "LU1CJILW0Hkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習過程の図示\n",
        "\n",
        "# Access the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot of the training and validation loss\n",
        "epochs = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot　of the training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aHdXnJvv5KwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) モデルの検証"
      ],
      "metadata": {
        "id": "AkmYSXfF3ExU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 検証データによる予測モデルの精度の検証\n",
        "score = model.evaluate(test_images, test_labels, verbose = 0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])"
      ],
      "metadata": {
        "id": "cNZgkgh222yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) モデルの保存と復元"
      ],
      "metadata": {
        "id": "bLshO9Gr5h4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存。HDF5ファイルで保存される。\n",
        "model.save('/content/drive/MyDrive/Telakoya/tela5/model/cnn.h5')"
      ],
      "metadata": {
        "id": "Sa6vspPz5ch0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの復元。\n",
        "from keras.models import load_model\n",
        "model2 = load_model('/content/drive/MyDrive/Telakoya/tela5/model/cnn.h5')"
      ],
      "metadata": {
        "id": "rRTTQuiC5xWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存前のモデルによる予測結果\n",
        "pred = model.predict(test_images[0:1])\n",
        "pred"
      ],
      "metadata": {
        "id": "UH5TZgW_5xWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 復元したモデルによる予測結果\n",
        "pred2 = model2.predict(test_images[0:1])\n",
        "pred2"
      ],
      "metadata": {
        "id": "HOzHBRZG5xWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. ChatGPTが提案したモデルによる分類"
      ],
      "metadata": {
        "id": "6qZWqpGVnSXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 分類精度の高いモデルの設計を ChatGPT に作らせた場合。"
      ],
      "metadata": {
        "id": "nXe-EHf8rcDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- OpenAI が公開している ChatGPT を利用する場合、以下のリンクから登録すれば誰でも利用できる。\n",
        "- GPT3.5から作られた ChatGPT は無利で使用できる。\n",
        "- $20/monthの ChatGPt Plusに加入すると GPT-4 も利用できる。(2023年5月時点)"
      ],
      "metadata": {
        "id": "qd9dUEf0rzCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chat.openai.com/"
      ],
      "metadata": {
        "id": "hOptfLt4rwq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Googe driveへのマウント。既に行われている場合は評価は不要。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cwf1VKoSq0c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) ライブラリの読み込み"
      ],
      "metadata": {
        "id": "LrBl5lGd6lIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu1t5p4InhOu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) データの読み込み"
      ],
      "metadata": {
        "id": "BqdDeC2Q6o0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Faoa9cnhO5"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- モデルの損失関数がSparse Categorical Crossentropyの場合、正解データは一次元でよい。"
      ],
      "metadata": {
        "id": "kPlneqW6u6LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) モデルの設計"
      ],
      "metadata": {
        "id": "_zIswdNe6xiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 【ChatGPTに入力したprompt】\n",
        "- I want to make a CNNN model using keras to classifiy cifar10 dataset with a high accuracy. Please sugget such a model."
      ],
      "metadata": {
        "id": "Ly88g4Zw64Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGPTが提案したモデル\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Convolutional layer 1\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "# Convolutional layer 2\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "# Convolutional layer 3\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "# Flatten the output\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "1B7jEHq_lhVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) モデルの学習"
      ],
      "metadata": {
        "id": "YmUZIlkv7m-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=25, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "GaSKqjEMAPVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの学習過程\n",
        "\n",
        "# Access the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot of the training and validation loss\n",
        "epochs = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot　of the training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VuurFYdX75sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) モデルの検証"
      ],
      "metadata": {
        "id": "5HLmeo_A8Yu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測モデルの精度をTest dataで検証\n",
        "score = model.evaluate(test_images, test_labels, verbose = 0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])"
      ],
      "metadata": {
        "id": "ESvfPuJf8bZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ChatGPTは、更なる予測精度の向上のため、以下のような suggestion も提案した。\n",
        "- 1. Data Augmentation データの水増し (反転、回転、移動など)\n",
        "- 2. より高度な ResNet, DenseNet, EfficientNet などの利用。\n",
        "- 3. Hyperparameterのチューニング\n",
        "- 4. 正則化 (regularization) L1, L2などで、次元削減を行う"
      ],
      "metadata": {
        "id": "b-42MSSWPFSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) モデルの保存と復元"
      ],
      "metadata": {
        "id": "jMqO20Oa9WsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存。HDF5ファイルで保存される。\n",
        "model.save('/content/drive/MyDrive/Telakoya/tela5/model/cnnC.h5')"
      ],
      "metadata": {
        "id": "gkIJJOQZ9l9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの復元。\n",
        "from keras.models import load_model\n",
        "model2 = load_model('/content/drive/MyDrive/Telakoya/tela5/model/cnnC.h5')"
      ],
      "metadata": {
        "id": "DAkKeL8G9l9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存前のモデルによる予測結果\n",
        "pred = model.predict(test_images[0:1])\n",
        "pred"
      ],
      "metadata": {
        "id": "R8GEWVp19l9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 復元したモデルによる予測結果\n",
        "pred2 = model2.predict(test_images[0:1])\n",
        "pred2"
      ],
      "metadata": {
        "id": "2iK7M4OB9l9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Kaggleで提案されたモデルによる分類"
      ],
      "metadata": {
        "id": "HYXivXvt-D1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 世界的な機械学習のCompetitionであるKaggleにおいて、Cifar10の画像分類の銀メダリストのKedar氏が提案したモデル。"
      ],
      "metadata": {
        "id": "YboBLqj9BEQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/kedarsai/cifar-10-88-accuracy-using-keras"
      ],
      "metadata": {
        "id": "FU854WNV_rtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/kedarsai"
      ],
      "metadata": {
        "id": "7qk4uz5Lu4q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Googe driveへのマウント。既に行われている場合は評価は不要。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XHHq3e_oq2js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) ライブラリの読み込み"
      ],
      "metadata": {
        "id": "rXAxrNCk-NtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 各種ライブラリの読み込み\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras.layers import BatchNormalization, MaxPool2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ydwxPEx--I6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) データの読み込み"
      ],
      "metadata": {
        "id": "BQjzHzND-kt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- モデルの損失関数が Cross Entropy のため、正解データは One Hot vectorに変換する"
      ],
      "metadata": {
        "id": "R5GBoTtX-9yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの分割\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# データの値が0〜1になるように正規化。\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# 正解データを One Hot vector に変換。\n",
        "from keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)"
      ],
      "metadata": {
        "id": "pUel5zQr-m_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) モデルの設計"
      ],
      "metadata": {
        "id": "gH2NERPl-n00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ８層からなるCNN"
      ],
      "metadata": {
        "id": "PFVcOG-num3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/kedarsai/cifar-10-88-accuracy-using-keras"
      ],
      "metadata": {
        "id": "W6-hCsQPu_oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KagglerのKedar氏が公開したモデル\n",
        "model6 = models.Sequential()\n",
        "model6.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(MaxPool2D((2, 2)))\n",
        "model6.add(layers.Dropout(0.2))\n",
        "model6.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(MaxPool2D((2, 2)))\n",
        "model6.add(layers.Dropout(0.3))\n",
        "model6.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(MaxPool2D((2, 2)))\n",
        "model6.add(layers.Dropout(0.4))\n",
        "model6.add(layers.Flatten())\n",
        "model6.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(layers.Dropout(0.5))\n",
        "model6.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model6.summary()"
      ],
      "metadata": {
        "id": "m7G6BgCd-pi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) モデルの学習"
      ],
      "metadata": {
        "id": "76P20KzQ-qZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- モデルが大きく、学習を200回行うため、計算に1時間以上かかる可能性あり。"
      ],
      "metadata": {
        "id": "FysA4CcIt3A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "# opt = SGD(lr=0.001, momentum=0.9)\n",
        "model6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Image Data Generator , we are shifting image accross width and height also we are flipping the image horizantally.\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True,rotation_range=20)\n",
        "it_train = datagen.flow(train_images, train_labels)\n",
        "steps = int(train_images.shape[0] / 64)\n",
        "\n",
        "# Be patient. It will take some time to train the due to 200 epochs of optimization.\n",
        "history6=model6.fit_generator(it_train, epochs=200, steps_per_epoch=steps, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "oalSqgPw-sQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの学習過程\n",
        "\n",
        "# Access the training history\n",
        "train_loss = history6.history['loss']\n",
        "val_loss = history6.history['val_loss']\n",
        "train_accuracy = history6.history['accuracy']\n",
        "val_accuracy = history6.history['val_accuracy']\n",
        "\n",
        "# Plot of the training and validation loss\n",
        "epochs = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot　of the training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p77auhLrCREY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) モデルの検証"
      ],
      "metadata": {
        "id": "M-Gv27So-tUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測モデルの精度をtest dataで検証\n",
        "score = model6.evaluate(test_images, test_labels, verbose = 0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])"
      ],
      "metadata": {
        "id": "vEzvTuvW-vK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) モデルの保存と復元"
      ],
      "metadata": {
        "id": "RBUnAOf4-wYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存。HDF5ファイルで保存される。モデルの名称のcnnKは変更可能。\n",
        "model6.save('/content/drive/MyDrive/Telakoya/tela5/model/cnnK.h5')"
      ],
      "metadata": {
        "id": "nA_m_izKCvNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの復元。\n",
        "from keras.models import load_model\n",
        "model62 = load_model('/content/drive/MyDrive/Telakoya/tela5/model/cnnK.h5')"
      ],
      "metadata": {
        "id": "VKMBhJs_CvNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存前のモデルによる予測結果\n",
        "pred = model6.predict(test_images[0:1])\n",
        "pred"
      ],
      "metadata": {
        "id": "KX4viuoTCvNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 復元したモデルによる予測結果\n",
        "pred2 = model62.predict(test_images[0:1])\n",
        "pred2"
      ],
      "metadata": {
        "id": "hMqbD88DCvNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}